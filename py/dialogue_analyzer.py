#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Ø£Ø¯ÙˆØ§Øª ØªØ­Ù„ÙŠÙ„ ÙˆÙ…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø­ÙˆØ§Ø±Ø§Øª ÙÙŠ Ø§Ù„Ø±ÙˆØ§ÙŠØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
Dialogue Analysis Tools for Arabic Novels
"""

import json
import re
from typing import List, Dict, Any, Tuple
from collections import Counter, defaultdict

class ArabicDialogueAnalyzer:
    """ÙØ¦Ø© Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø­ÙˆØ§Ø±Ø§Øª ÙÙŠ Ø§Ù„Ø±ÙˆØ§ÙŠØ§Øª Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©"""
    
    def __init__(self, dialogues_file: str = None):
        """
        ØªÙ‡ÙŠØ¦Ø© Ù…Ø­Ù„Ù„ Ø§Ù„Ø­ÙˆØ§Ø±Ø§Øª
        
        Args:
            dialogues_file: Ù…Ø³Ø§Ø± Ù…Ù„Ù JSONL ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ø­ÙˆØ§Ø±Ø§Øª
        """
        self.dialogues = []
        if dialogues_file:
            self.load_dialogues(dialogues_file)
    
    def load_dialogues(self, file_path: str):
        """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø­ÙˆØ§Ø±Ø§Øª Ù…Ù† Ù…Ù„Ù JSONL"""
        self.dialogues = []
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    self.dialogues.append(json.loads(line))
        print(f"âœ“ ØªÙ… ØªØ­Ù…ÙŠÙ„ {len(self.dialogues)} Ø­ÙˆØ§Ø±")
    
    def get_character_speech_stats(self, character_id: str) -> Dict[str, Any]:
        """
        Ø§Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ÙƒÙ„Ø§Ù… Ù„Ø´Ø®ØµÙŠØ© Ù…Ø¹ÙŠÙ†Ø©
        
        Args:
            character_id: Ù…Ø¹Ø±Ù Ø§Ù„Ø´Ø®ØµÙŠØ©
            
        Returns:
            Ù‚Ø§Ù…ÙˆØ³ Ø¨Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
        """
        total_lines = 0
        total_words = 0
        emotions = []
        languages = []
        intensities = []
        
        for dialogue in self.dialogues:
            if character_id not in dialogue.get('participants', []):
                continue
            
            for exchange in dialogue.get('exchanges', []):
                if exchange.get('speaker') == character_id:
                    total_lines += 1
                    text = exchange.get('text', '')
                    total_words += len(text.split())
                    
                    if 'emotion' in exchange:
                        emotions.append(exchange['emotion'])
                    if 'lang' in exchange:
                        languages.append(exchange['lang'])
                    if 'intensity' in exchange:
                        intensities.append(exchange['intensity'])
        
        stats = {
            'total_dialogues_participated': len([d for d in self.dialogues 
                                                 if character_id in d.get('participants', [])]),
            'total_lines_spoken': total_lines,
            'total_words_spoken': total_words,
            'avg_words_per_line': total_words / total_lines if total_lines > 0 else 0,
            'emotion_distribution': Counter(emotions),
            'language_distribution': Counter(languages),
            'avg_emotional_intensity': sum(intensities) / len(intensities) if intensities else 0
        }
        
        return stats
    
    def analyze_relationship_from_dialogues(self, char1_id: str, char2_id: str) -> Dict[str, Any]:
        """
        ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¹Ù„Ø§Ù‚Ø© Ø¨ÙŠÙ† Ø´Ø®ØµÙŠØªÙŠÙ† Ù…Ù† Ø®Ù„Ø§Ù„ Ø­ÙˆØ§Ø±Ø§ØªÙ‡Ù…Ø§
        
        Args:
            char1_id: Ù…Ø¹Ø±Ù Ø§Ù„Ø´Ø®ØµÙŠØ© Ø§Ù„Ø£ÙˆÙ„Ù‰
            char2_id: Ù…Ø¹Ø±Ù Ø§Ù„Ø´Ø®ØµÙŠØ© Ø§Ù„Ø«Ø§Ù†ÙŠØ©
            
        Returns:
            ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¹Ù„Ø§Ù‚Ø©
        """
        shared_dialogues = [
            d for d in self.dialogues 
            if char1_id in d.get('participants', []) and char2_id in d.get('participants', [])
        ]
        
        if not shared_dialogues:
            return {"relationship": "Ù„Ø§ ØªÙˆØ¬Ø¯ Ø­ÙˆØ§Ø±Ø§Øª Ù…Ø´ØªØ±ÙƒØ©"}
        
        total_exchanges = sum(len(d.get('exchanges', [])) for d in shared_dialogues)
        char1_lines = 0
        char2_lines = 0
        emotions_char1 = []
        emotions_char2 = []
        
        for dialogue in shared_dialogues:
            for exchange in dialogue.get('exchanges', []):
                speaker = exchange.get('speaker')
                if speaker == char1_id:
                    char1_lines += 1
                    if 'emotion' in exchange:
                        emotions_char1.append(exchange['emotion'])
                elif speaker == char2_id:
                    char2_lines += 1
                    if 'emotion' in exchange:
                        emotions_char2.append(exchange['emotion'])
        
        analysis = {
            'shared_dialogues_count': len(shared_dialogues),
            'total_exchanges': total_exchanges,
            'balance': {
                'char1_lines': char1_lines,
                'char2_lines': char2_lines,
                'balance_ratio': char1_lines / char2_lines if char2_lines > 0 else 0
            },
            'emotional_profile': {
                'char1_emotions': Counter(emotions_char1),
                'char2_emotions': Counter(emotions_char2)
            },
            'dialogue_types': Counter(d.get('type', 'unknown') for d in shared_dialogues),
            'intimacy_indicator': len(shared_dialogues) / len(self.dialogues) if self.dialogues else 0
        }
        
        return analysis
    
    def detect_language_patterns(self, character_id: str) -> Dict[str, Any]:
        """
        ÙƒØ´Ù Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù„ØºØ© Ù„Ø´Ø®ØµÙŠØ©
        
        Args:
            character_id: Ù…Ø¹Ø±Ù Ø§Ù„Ø´Ø®ØµÙŠØ©
            
        Returns:
            Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù„ØºØ©
        """
        fsa_count = 0
        colloquial_count = 0
        mixed_count = 0
        
        code_switching_patterns = []
        
        for dialogue in self.dialogues:
            if character_id not in dialogue.get('participants', []):
                continue
            
            prev_lang = None
            for exchange in dialogue.get('exchanges', []):
                if exchange.get('speaker') == character_id:
                    lang = exchange.get('lang', 'unknown')
                    
                    if lang == 'fsa':
                        fsa_count += 1
                    elif lang == 'colloquial':
                        colloquial_count += 1
                    elif lang == 'mixed':
                        mixed_count += 1
                    
                    # ÙƒØ´Ù Ø§Ù„ØªØ¨Ø¯ÙŠÙ„ Ø§Ù„Ù„ØºÙˆÙŠ
                    if prev_lang and prev_lang != lang:
                        code_switching_patterns.append({
                            'from': prev_lang,
                            'to': lang,
                            'context': exchange.get('emotion', 'unknown')
                        })
                    
                    prev_lang = lang
        
        total = fsa_count + colloquial_count + mixed_count
        
        patterns = {
            'language_distribution': {
                'fsa': fsa_count,
                'colloquial': colloquial_count,
                'mixed': mixed_count
            },
            'language_percentages': {
                'fsa': fsa_count / total if total > 0 else 0,
                'colloquial': colloquial_count / total if total > 0 else 0,
                'mixed': mixed_count / total if total > 0 else 0
            },
            'code_switching_frequency': len(code_switching_patterns),
            'code_switching_patterns': code_switching_patterns[:5]  # Ø£ÙˆÙ„ 5 Ø£Ù…Ø«Ù„Ø©
        }
        
        return patterns
    
    def extract_memorable_quotes(self, min_intensity: float = 0.80) -> List[Dict[str, Any]]:
        """
        Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø§Ù‚ØªØ¨Ø§Ø³Ø§Øª Ø§Ù„Ù…Ù…ÙŠØ²Ø©
        
        Args:
            min_intensity: Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ø¯Ù†Ù‰ Ù„Ù„Ø´Ø¯Ø© Ø§Ù„Ø¹Ø§Ø·ÙÙŠØ©
            
        Returns:
            Ù‚Ø§Ø¦Ù…Ø© Ø¨Ø§Ù„Ø§Ù‚ØªØ¨Ø§Ø³Ø§Øª
        """
        quotes = []
        
        for dialogue in self.dialogues:
            for exchange in dialogue.get('exchanges', []):
                intensity = exchange.get('intensity', 0)
                
                if intensity >= min_intensity:
                    quotes.append({
                        'text': exchange.get('text'),
                        'speaker': exchange.get('speaker'),
                        'emotion': exchange.get('emotion'),
                        'intensity': intensity,
                        'dialogue_id': dialogue.get('dialogue_id'),
                        'chapter': dialogue.get('chapter')
                    })
        
        # ØªØ±ØªÙŠØ¨ Ø­Ø³Ø¨ Ø§Ù„Ø´Ø¯Ø©
        quotes.sort(key=lambda x: x['intensity'], reverse=True)
        
        return quotes
    
    def analyze_emotional_arc(self, dialogue_id: str) -> Dict[str, Any]:
        """
        ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ø¹Ø§Ø·ÙÙŠ Ù„Ù„Ø­ÙˆØ§Ø±
        
        Args:
            dialogue_id: Ù…Ø¹Ø±Ù Ø§Ù„Ø­ÙˆØ§Ø±
            
        Returns:
            ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ø¹Ø§Ø·ÙÙŠ
        """
        dialogue = next((d for d in self.dialogues if d.get('dialogue_id') == dialogue_id), None)
        
        if not dialogue:
            return {"error": "Ø§Ù„Ø­ÙˆØ§Ø± ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯"}
        
        exchanges = dialogue.get('exchanges', [])
        
        emotional_trajectory = []
        for i, exchange in enumerate(exchanges):
            emotional_trajectory.append({
                'exchange_number': i + 1,
                'emotion': exchange.get('emotion'),
                'intensity': exchange.get('intensity', 0),
                'speaker': exchange.get('speaker')
            })
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØµØ§Ø¹Ø¯/Ø§Ù„ØªÙ†Ø§Ø²Ù„ Ø§Ù„Ø¹Ø§Ø·ÙÙŠ
        intensities = [e.get('intensity', 0) for e in exchanges]
        
        arc_analysis = {
            'emotional_trajectory': emotional_trajectory,
            'start_intensity': intensities[0] if intensities else 0,
            'peak_intensity': max(intensities) if intensities else 0,
            'end_intensity': intensities[-1] if intensities else 0,
            'peak_exchange_number': intensities.index(max(intensities)) + 1 if intensities else 0,
            'arc_type': self._determine_arc_type(intensities)
        }
        
        return arc_analysis
    
    def _determine_arc_type(self, intensities: List[float]) -> str:
        """ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ø¹Ø§Ø·ÙÙŠ"""
        if not intensities or len(intensities) < 3:
            return "Ù‚ØµÙŠØ± Ø¬Ø¯Ø§Ù‹ Ù„Ù„ØªØ­Ù„ÙŠÙ„"
        
        start = intensities[0]
        peak = max(intensities)
        end = intensities[-1]
        
        if peak == intensities[-1]:
            return "ØªØµØ§Ø¹Ø¯ÙŠ"
        elif peak == intensities[0]:
            return "ØªÙ†Ø§Ø²Ù„ÙŠ"
        elif start < peak > end:
            return "Ù‚ÙˆØ³ (ØªØµØ§Ø¹Ø¯ Ø«Ù… ØªÙ†Ø§Ø²Ù„)"
        else:
            return "Ù…ØªÙ‚Ù„Ø¨"
    
    def get_dialogue_statistics(self) -> Dict[str, Any]:
        """Ø§Ø­ØµØ§Ø¦ÙŠØ§Øª Ø¹Ø§Ù…Ø© Ø¹Ù† Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø­ÙˆØ§Ø±Ø§Øª"""
        if not self.dialogues:
            return {}
        
        types = Counter(d.get('type', 'unknown') for d in self.dialogues)
        importance_levels = Counter(d.get('importance', 'unknown') for d in self.dialogues)
        
        all_emotions = []
        all_languages = []
        
        for dialogue in self.dialogues:
            for exchange in dialogue.get('exchanges', []):
                if 'emotion' in exchange:
                    all_emotions.append(exchange['emotion'])
                if 'lang' in exchange:
                    all_languages.append(exchange['lang'])
        
        stats = {
            'total_dialogues': len(self.dialogues),
            'dialogue_types': dict(types),
            'importance_levels': dict(importance_levels),
            'emotion_distribution': dict(Counter(all_emotions).most_common(10)),
            'language_distribution': dict(Counter(all_languages)),
            'avg_exchanges_per_dialogue': sum(len(d.get('exchanges', [])) 
                                             for d in self.dialogues) / len(self.dialogues)
        }
        
        return stats
    
    def find_dialogues_by_emotion(self, emotion: str) -> List[Dict[str, Any]]:
        """Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø­ÙˆØ§Ø±Ø§Øª ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¹Ø§Ø·ÙØ© Ù…Ø¹ÙŠÙ†Ø©"""
        matching_dialogues = []
        
        for dialogue in self.dialogues:
            for exchange in dialogue.get('exchanges', []):
                if exchange.get('emotion', '').lower() == emotion.lower():
                    matching_dialogues.append({
                        'dialogue_id': dialogue.get('dialogue_id'),
                        'chapter': dialogue.get('chapter'),
                        'text': exchange.get('text'),
                        'speaker': exchange.get('speaker')
                    })
                    break
        
        return matching_dialogues
    
    def export_for_training(self, output_file: str, task_type: str = 'generation'):
        """
        ØªØµØ¯ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨ØµÙŠØºØ© Ø¬Ø§Ù‡Ø²Ø© Ù„Ù„ØªØ¯Ø±ÙŠØ¨
        
        Args:
            output_file: Ù…Ù„Ù Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬
            task_type: Ù†ÙˆØ¹ Ø§Ù„Ù…Ù‡Ù…Ø© (generation, emotion_recognition, etc.)
        """
        training_data = []
        
        for dialogue in self.dialogues:
            if task_type == 'generation':
                # ØªÙ†Ø³ÙŠÙ‚ Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø­ÙˆØ§Ø±Ø§Øª
                context = f"ÙÙŠ {dialogue.get('chapter', 'ÙØµÙ„')}, Ù†ÙˆØ¹ Ø§Ù„Ø­ÙˆØ§Ø±: {dialogue.get('type', 'Ø¹Ø§Ù…')}"
                exchanges_text = "\n".join([
                    f"{ex.get('speaker', 'Ù…ØªØ­Ø¯Ø«')}: {ex.get('text', '')}"
                    for ex in dialogue.get('exchanges', [])
                ])
                
                training_data.append({
                    "prompt": f"Ø§ÙƒØªØ¨ Ø­ÙˆØ§Ø±Ø§Ù‹ {dialogue.get('type', 'Ø¹Ø§Ù…Ø§Ù‹')} ÙÙŠ Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„ØªØ§Ù„ÙŠ: {context}",
                    "completion": exchanges_text
                })
            
            elif task_type == 'emotion_recognition':
                # ØªÙ†Ø³ÙŠÙ‚ Ù„Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±
                for exchange in dialogue.get('exchanges', []):
                    training_data.append({
                        "text": exchange.get('text', ''),
                        "emotion": exchange.get('emotion', 'unknown'),
                        "intensity": exchange.get('intensity', 0)
                    })
        
        with open(output_file, 'w', encoding='utf-8') as f:
            for item in training_data:
                f.write(json.dumps(item, ensure_ascii=False) + '\n')
        
        print(f"âœ“ ØªÙ… ØªØµØ¯ÙŠØ± {len(training_data)} Ø¹Ù†ØµØ± ØªØ¯Ø±ÙŠØ¨ Ø¥Ù„Ù‰ {output_file}")


def generate_dialogue_prompt(context: Dict[str, Any]) -> str:
    """
    ØªÙˆÙ„ÙŠØ¯ prompt Ù„ØªÙˆÙ„ÙŠØ¯ Ø­ÙˆØ§Ø±
    
    Args:
        context: Ø³ÙŠØ§Ù‚ Ø§Ù„Ø­ÙˆØ§Ø±
        
    Returns:
        prompt Ø¬Ø§Ù‡Ø²
    """
    participants = context.get('participants', [])
    setting = context.get('setting', 'Ù…ÙƒØ§Ù† ØºÙŠØ± Ù…Ø­Ø¯Ø¯')
    conflict = context.get('conflict', 'ØµØ±Ø§Ø¹ ØºÙŠØ± Ù…Ø­Ø¯Ø¯')
    
    prompt = f"""Ø§ÙƒØªØ¨ Ø­ÙˆØ§Ø±Ø§Ù‹ Ø¨ÙŠÙ† {' Ùˆ '.join([p['name'] for p in participants])}.

Ø§Ù„Ø³ÙŠØ§Ù‚:
- Ø§Ù„Ù…ÙƒØ§Ù†: {setting}
- Ø§Ù„ØµØ±Ø§Ø¹: {conflict}

Ø§Ù„Ø´Ø®ØµÙŠØ§Øª:
"""
    
    for p in participants:
        prompt += f"- {p['name']}: {p.get('description', 'Ø´Ø®ØµÙŠØ©')}, Ø­Ø§Ù„ØªÙ‡ Ø§Ù„Ø¹Ø§Ø·ÙÙŠØ©: {p.get('emotion', 'Ù…Ø­Ø§ÙŠØ¯')}\n"
    
    prompt += "\nØ§ÙƒØªØ¨ Ø§Ù„Ø­ÙˆØ§Ø±:"
    
    return prompt


# Ù…Ø«Ø§Ù„ Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
if __name__ == "__main__":
    print("="*60)
    print("Ù…Ø­Ù„Ù„ Ø§Ù„Ø­ÙˆØ§Ø±Ø§Øª Ù„Ù„Ø±ÙˆØ§ÙŠØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©")
    print("="*60)
    
    # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø­ÙˆØ§Ø±Ø§Øª
    analyzer = ArabicDialogueAnalyzer('dialogues_dataset.jsonl')
    
    # Ø§Ø­ØµØ§Ø¦ÙŠØ§Øª Ø¹Ø§Ù…Ø©
    print("\nğŸ“Š Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø¹Ø§Ù…Ø©:")
    stats = analyzer.get_dialogue_statistics()
    print(f"Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø­ÙˆØ§Ø±Ø§Øª: {stats.get('total_dialogues', 0)}")
    print(f"Ù…ØªÙˆØ³Ø· Ø§Ù„ØªØ¨Ø§Ø¯Ù„Ø§Øª Ù„ÙƒÙ„ Ø­ÙˆØ§Ø±: {stats.get('avg_exchanges_per_dialogue', 0):.1f}")
    
    print("\nğŸ­ ØªÙˆØ²ÙŠØ¹ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø­ÙˆØ§Ø±Ø§Øª:")
    for dtype, count in stats.get('dialogue_types', {}).items():
        print(f"  {dtype}: {count}")
    
    print("\nğŸ˜Š Ø£ÙƒØ«Ø± Ø§Ù„Ù…Ø´Ø§Ø¹Ø± Ø´ÙŠÙˆØ¹Ø§Ù‹:")
    for emotion, count in list(stats.get('emotion_distribution', {}).items())[:5]:
        print(f"  {emotion}: {count}")
    
    # ØªØ­Ù„ÙŠÙ„ Ø´Ø®ØµÙŠØ©
    print("\nğŸ‘¤ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø´Ø®ØµÙŠØ© char_001:")
    char_stats = analyzer.get_character_speech_stats('char_001')
    print(f"Ø¹Ø¯Ø¯ Ø§Ù„Ø­ÙˆØ§Ø±Ø§Øª Ø§Ù„Ù…Ø´Ø§Ø±ÙƒØ© ÙÙŠÙ‡Ø§: {char_stats['total_dialogues_participated']}")
    print(f"Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø£Ø³Ø·Ø±: {char_stats['total_lines_spoken']}")
    print(f"Ù…ØªÙˆØ³Ø· Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ù„ÙƒÙ„ Ø³Ø·Ø±: {char_stats['avg_words_per_line']:.1f}")
    
    # Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù„ØºØ©
    print("\nğŸ—£ï¸ Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù„ØºØ© Ù„Ù„Ø´Ø®ØµÙŠØ© char_001:")
    lang_patterns = analyzer.detect_language_patterns('char_001')
    print(f"Ø§Ù„ÙØµØ­Ù‰: {lang_patterns['language_percentages']['fsa']*100:.1f}%")
    print(f"Ø§Ù„Ø¹Ø§Ù…ÙŠØ©: {lang_patterns['language_percentages']['colloquial']*100:.1f}%")
    print(f"Ù…Ø®ØªÙ„Ø·: {lang_patterns['language_percentages']['mixed']*100:.1f}%")
    
    # Ø§Ù‚ØªØ¨Ø§Ø³Ø§Øª Ù…Ù…ÙŠØ²Ø©
    print("\nğŸ’¬ Ø£Ù‚ÙˆÙ‰ 3 Ø§Ù‚ØªØ¨Ø§Ø³Ø§Øª Ø¹Ø§Ø·ÙÙŠØ§Ù‹:")
    quotes = analyzer.extract_memorable_quotes(min_intensity=0.80)
    for i, quote in enumerate(quotes[:3], 1):
        print(f"{i}. \"{quote['text']}\"")
        print(f"   (Ø§Ù„Ø´Ø¯Ø©: {quote['intensity']}, Ø§Ù„Ù…Ø´Ø§Ø¹Ø±: {quote['emotion']})")
    
    # ØªØ­Ù„ÙŠÙ„ Ø¹Ù„Ø§Ù‚Ø©
    print("\nâ¤ï¸ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¹Ù„Ø§Ù‚Ø© Ø¨ÙŠÙ† char_001 Ùˆ char_003:")
    relationship = analyzer.analyze_relationship_from_dialogues('char_001', 'char_003')
    print(f"Ø¹Ø¯Ø¯ Ø§Ù„Ø­ÙˆØ§Ø±Ø§Øª Ø§Ù„Ù…Ø´ØªØ±ÙƒØ©: {relationship.get('shared_dialogues_count', 0)}")
    print(f"Ù†Ø³Ø¨Ø© Ø§Ù„ØªÙˆØ§Ø²Ù†: {relationship['balance']['balance_ratio']:.2f}")
    
    print("\nâœ… Ø§Ù†ØªÙ‡Ù‰ Ø§Ù„ØªØ­Ù„ÙŠÙ„!")
